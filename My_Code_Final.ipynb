{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "import bs4 \n",
    "import faiss\n",
    "import numpy as np\n",
    "from langchain import hub\n",
    "#from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import joblib\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "api_key= os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_key\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded and preprocessed successfully!\n",
      "Accuracy: 0.6783536585365854\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.86      0.77       412\n",
      "           1       0.61      0.38      0.47       244\n",
      "\n",
      "    accuracy                           0.68       656\n",
      "   macro avg       0.65      0.62      0.62       656\n",
      "weighted avg       0.67      0.68      0.66       656\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['water_potability_model.pkl']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset\n",
    "data1 = pd.read_csv(\"water_potability.csv\")\n",
    "#data2 = pd.read_csv(\"waterQuality1.csv\")\n",
    "# Optionally, concatenate them into a single dataset (if they have the same structure)\n",
    "#data1 = pd.concat([data1, data2], ignore_index=True)\n",
    "#combined_data.replace(\"#NUM!\", np.nan, inplace=True)\n",
    "\n",
    "# Preview the data\n",
    "data1.head()\n",
    "\n",
    "# Handle missing values by replacing them with the mean of each column\n",
    "imputer = SimpleImputer(strategy=\"mean\")\n",
    "data1.iloc[:, :-1] = imputer.fit_transform(data1.iloc[:, :-1])\n",
    "# Verify missing values are handled\n",
    "data1.isnull().sum()\n",
    "\n",
    "# Split dataset into features (X) and target (y)\n",
    "X = data1.drop(columns=[\"Potability\"])\n",
    "y = data1[\"Potability\"]\n",
    "\n",
    "# Split dataset into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features (normalize the data)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "print(\"Data loaded and preprocessed successfully!\")\n",
    "# Train a Random Forest Classifier\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Save the trained model to disk\n",
    "joblib.dump(model, \"water_potability_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model when needed\n",
    "model = joblib.load(\"water_potability_model.pkl\")\n",
    "\n",
    "# Function to predict water potability based on input features\n",
    "def predict_potability(features):\n",
    "    \"\"\"\n",
    "    Predict water potability based on input features.\n",
    "    \n",
    "    Args:\n",
    "    - features (list): A list of feature values in the same order as the dataset.\n",
    "    \n",
    "    Returns:\n",
    "    - str: Prediction result (\"Potable\" or \"Not Potable\").\n",
    "    \"\"\"\n",
    "    prediction = model.predict([features])[0]\n",
    "    return \"Potable\" if prediction == 1 else \"Not Potable\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents loaded and split into chunks successfully!\n"
     ]
    }
   ],
   "source": [
    "# Define websites to scrape information from\n",
    "websites = [\n",
    "    \"http://environnement.wallonie.be/de/eso/eau_distribution/\",\n",
    "    \"https://environment.ec.europa.eu/topics/water/water-wise-eu/belgium_en\",\n",
    "    \"https://environment.ec.europa.eu/topics/water/water-wise-eu/polluted-water_en\",\n",
    "    \"https://www.brusselstimes.com/1009591/flemish-drinking-water-highly-polluted-with-pfas-but-purifying-costs-millions\"\n",
    "]\n",
    "\n",
    "# Load and scrape the content of the websites\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=websites,\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\"p\")\n",
    "    )\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "# Split the documents into smaller chunks for easier processing\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "print(\"Documents loaded and split into chunks successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents embedded and indexed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Embed the document chunks using OpenAI embeddings\n",
    "embedding_model = OpenAIEmbeddings()\n",
    "embeddings = embedding_model.embed_documents([split.page_content for split in splits])\n",
    "\n",
    "# Initialize a FAISS index to store and search the embeddings\n",
    "dimension = len(embeddings[0])  # Get the dimension of the embeddings\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(np.array(embeddings, dtype='float32'))  # Add embeddings to the index\n",
    "\n",
    "print(\"Documents embedded and indexed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval function defined successfully!\n"
     ]
    }
   ],
   "source": [
    "# Function to retrieve the most similar documents for a given query\n",
    "def retrieve_similar(query, k=5):\n",
    "    query_embedding = np.array([embedding_model.embed_query(query)], dtype='float32')\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "    return [splits[i] for i in indices[0]]\n",
    "\n",
    "print(\"Retrieval function defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt templates defined successfully!\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Define different prompt templates for response generation\n",
    "general_prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"Using the information provided:\\n{context}\\nAnswer the question:\\n{question}\"\n",
    ")\n",
    "\n",
    "detailed_analysis_prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"Based on the following detailed analysis:\\n{context}\\nProvide a comprehensive answer to the question:\\n{question}\"\n",
    ")\n",
    "\n",
    "summarized_response_prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"Summarize the following content:\\n{context}\\nAnswer briefly:\\n{question}\"\n",
    ")\n",
    "\n",
    "# Dictionary to manage prompt choices\n",
    "prompt_choices = {\n",
    "    \"general\": general_prompt,\n",
    "    \"detailed\": detailed_analysis_prompt,\n",
    "    \"summary\": summarized_response_prompt\n",
    "}\n",
    "\n",
    "print(\"Prompt templates defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MediaMonster\\AppData\\Local\\Temp\\ipykernel_1708\\3191499033.py:6: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  rag_chain = LLMChain(llm=llm, prompt=prompt)\n",
      "C:\\Users\\MediaMonster\\AppData\\Local\\Temp\\ipykernel_1708\\3191499033.py:14: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = rag_chain.run({\"context\": context, \"question\": \"What is the potability of water with these characteristics?\"})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the information provided, the potability of the water can be evaluated as follows:\n",
      "\n",
      "- The water should not contain any microorganisms, parasites, or substances that pose a potential health risk.\n",
      "- It should meet the values parametric standards set by the Water Code.\n",
      "- The three main categories of parameters to consider are microbiological, chemical, and indicator parameters.\n",
      "- Non-conformities in water quality may be related to fecal pollution indicators, acidity levels, and other quality issues like chlorine excess or metal concentrations.\n",
      "- The presence of PFAS in drinking water in Flanders has raised concerns about water safety.\n",
      "- The technology to remove PFAS from water is not fully developed and can be expensive, leading to increased costs for purification.\n",
      "\n",
      "Overall, the potability of water with these characteristics may be compromised due to various factors such as contamination, high levels of PFAS, and the need for advanced purification methods. Further testing and treatment may be necessary to ensure the water meets safety standards.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Define a simpler RAG chain\n",
    "prompt = PromptTemplate(template=\"{context}\\n\\nQ: {question}\\nA:\", input_variables=[\"context\", \"question\"])\n",
    "rag_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Format documents manually\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Ask a question\n",
    "context = format_docs(retrieve_similar(\"What is the potability of water with these characteristics?\"))\n",
    "response = rag_chain.run({\"context\": context, \"question\": \"What is the potability of water with these characteristics?\"})\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The water described in the text meets the requirements for potability, as it is free from microorganisms, parasites, and other substances that could be harmful to health. It also conforms to the values set for potable water. Therefore, the water can be considered potable.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def predict_potability(features):\n",
    "    # Mock prediction logic (replace with your ML model or calculation)\n",
    "    return \"Potable\" if features[0] > 7.0 else \"Not Potable\"\n",
    "\n",
    "# Function to format context with potability prediction\n",
    "def format_with_predictions(docs, features=None):\n",
    "    context = \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "    if features:\n",
    "        potability = predict_potability(features)  # Assuming `predict_potability` is defined\n",
    "        context += f\"\\n\\nWater Potability Prediction: {potability}\"\n",
    "    return context\n",
    "\n",
    "# Define the prompt template\n",
    "prompt_template = PromptTemplate(\n",
    "    template=\"{context}\\n\\nQ: {question}\\nA:\",\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# Define the chain\n",
    "rag_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "# Retrieve documents and generate response\n",
    "docs = retrieve_similar(\"What is the potability of water with these characteristics?\")\n",
    "context = format_with_predictions(docs, features=[7.2, 204.5, 20791, 7.2, 333.3, 17.2, 6.2, 325.3, 0.5])\n",
    "response = rag_chain.run({\"context\": context, \"question\": \"What is the potability of water with these characteristics?\"})\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
