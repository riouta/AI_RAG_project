{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
>>>>>>> d03e42cc7351facd3f7091404dff73b6adec4af1
   "source": [
    "import getpass\n",
    "import os\n",
    "import bs4 \n",
    "import faiss\n",
    "import numpy as np\n",
    "from langchain import hub\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import joblib\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "api_key= os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_key\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 'Chat' object has no attribute 'Completion'\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "openai.api_key = 'OPENAI_API_KEY'\n",
    "\n",
    "try:\n",
    "    response = openai.chat.Completion.create(\n",
    "        model=\"gpt-3.5-turbo\", \n",
    "        prompt=\"Say hello!\", \n",
    "        max_tokens=5\n",
    "    )\n",
    "    print(response)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded and preprocessed successfully!\n"
     ]
    }
   ],
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> d03e42cc7351facd3f7091404dff73b6adec4af1
   "source": [
    "# Load dataset\n",
    "data1 = pd.read_csv(\"water_potability.csv\")\n",
    "#data2 = pd.read_csv(\"waterQuality1.csv\")\n",
    "# Optionally, concatenate them into a single dataset (if they have the same structure)\n",
    "#data1 = pd.concat([data1, data2], ignore_index=True)\n",
    "#combined_data.replace(\"#NUM!\", np.nan, inplace=True)\n",
    "\n",
    "# Preview the data\n",
    "data1.head()\n",
    "\n",
    "# Handle missing values by replacing them with the mean of each column\n",
    "imputer = SimpleImputer(strategy=\"mean\")\n",
    "data1.iloc[:, :-1] = imputer.fit_transform(data1.iloc[:, :-1])\n",
    "# Verify missing values are handled\n",
    "data1.isnull().sum()\n",
    "\n",
    "# Split dataset into features (X) and target (y)\n",
    "X = data1.drop(columns=[\"Potability\"])\n",
    "y = data1[\"Potability\"]\n",
    "\n",
    "# Split dataset into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features (normalize the data)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "print(\"Data loaded and preprocessed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6783536585365854\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.86      0.77       412\n",
      "           1       0.61      0.38      0.47       244\n",
      "\n",
      "    accuracy                           0.68       656\n",
      "   macro avg       0.65      0.62      0.62       656\n",
      "weighted avg       0.67      0.68      0.66       656\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['water_potability_model.pkl']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> d03e42cc7351facd3f7091404dff73b6adec4af1
   "source": [
    "# Train a Random Forest Classifier\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Save the trained model to disk\n",
    "joblib.dump(model, \"water_potability_model.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 20,
=======
   "execution_count": null,
>>>>>>> d03e42cc7351facd3f7091404dff73b6adec4af1
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model when needed\n",
    "model = joblib.load(\"water_potability_model.pkl\")\n",
    "\n",
    "# Function to predict water potability based on input features\n",
    "def predict_potability(features):\n",
    "    \"\"\"\n",
    "    Predict water potability based on input features.\n",
    "    \n",
    "    Args:\n",
    "    - features (list): A list of feature values in the same order as the dataset.\n",
    "    \n",
    "    Returns:\n",
    "    - str: Prediction result (\"Potable\" or \"Not Potable\").\n",
    "    \"\"\"\n",
    "    prediction = model.predict([features])[0]\n",
    "    return \"Potable\" if prediction == 1 else \"Not Potable\"\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents loaded and split into chunks successfully!\n"
     ]
    }
   ],
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> d03e42cc7351facd3f7091404dff73b6adec4af1
   "source": [
    "# Define websites to scrape information from\n",
    "websites = [\n",
    "    \"http://environnement.wallonie.be/de/eso/eau_distribution/\",\n",
    "    \"https://environment.ec.europa.eu/topics/water/water-wise-eu/belgium_en\",\n",
    "    \"https://environment.ec.europa.eu/topics/water/water-wise-eu/polluted-water_en\",\n",
    "    \"https://www.brusselstimes.com/1009591/flemish-drinking-water-highly-polluted-with-pfas-but-purifying-costs-millions\"\n",
    "]\n",
    "\n",
    "# Load and scrape the content of the websites\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=websites,\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\"p\")\n",
    "    )\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "# Split the documents into smaller chunks for easier processing\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "print(\"Documents loaded and split into chunks successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "APIConnectionError",
     "evalue": "Connection error.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConnectError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\MediaMonster\\OneDrive\\MA2\\AI\\Project_AI\\AI_RAG_project\\myenv\\Lib\\site-packages\\httpx\\_transports\\default.py:72\u001b[0m, in \u001b[0;36mmap_httpcore_exceptions\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\MediaMonster\\OneDrive\\MA2\\AI\\Project_AI\\AI_RAG_project\\myenv\\Lib\\site-packages\\httpx\\_transports\\default.py:236\u001b[0m, in \u001b[0;36mhandle_request\u001b[1;34m(self, request)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\MediaMonster\\OneDrive\\MA2\\AI\\Project_AI\\AI_RAG_project\\myenv\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:256\u001b[0m, in \u001b[0;36mhandle_request\u001b[1;34m(self, request)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\MediaMonster\\OneDrive\\MA2\\AI\\Project_AI\\AI_RAG_project\\myenv\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:236\u001b[0m, in \u001b[0;36mhandle_request\u001b[1;34m(self, request)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\MediaMonster\\OneDrive\\MA2\\AI\\Project_AI\\AI_RAG_project\\myenv\\Lib\\site-packages\\httpcore\\_sync\\connection.py:101\u001b[0m, in \u001b[0;36mhandle_request\u001b[1;34m(self, request)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\MediaMonster\\OneDrive\\MA2\\AI\\Project_AI\\AI_RAG_project\\myenv\\Lib\\site-packages\\httpcore\\_sync\\connection.py:78\u001b[0m, in \u001b[0;36mhandle_request\u001b[1;34m(self, request)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\MediaMonster\\OneDrive\\MA2\\AI\\Project_AI\\AI_RAG_project\\myenv\\Lib\\site-packages\\httpcore\\_sync\\connection.py:156\u001b[0m, in \u001b[0;36m_connect\u001b[1;34m(self, request)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\MediaMonster\\OneDrive\\MA2\\AI\\Project_AI\\AI_RAG_project\\myenv\\Lib\\site-packages\\httpcore\\_backends\\sync.py:154\u001b[0m, in \u001b[0;36mstart_tls\u001b[1;34m(self, ssl_context, server_hostname, timeout)\u001b[0m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2032.0_x64__qbz5n2kfra8p0\\Lib\\contextlib.py:158\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[1;34m(self, typ, value, traceback)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 158\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    160\u001b[0m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\MediaMonster\\OneDrive\\MA2\\AI\\Project_AI\\AI_RAG_project\\myenv\\Lib\\site-packages\\httpcore\\_exceptions.py:14\u001b[0m, in \u001b[0;36mmap_exceptions\u001b[1;34m(map)\u001b[0m\n",
      "\u001b[1;31mConnectError\u001b[0m: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mConnectError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\MediaMonster\\OneDrive\\MA2\\AI\\Project_AI\\AI_RAG_project\\myenv\\Lib\\site-packages\\openai\\_base_client.py:991\u001b[0m, in \u001b[0;36m_request\u001b[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\MediaMonster\\OneDrive\\MA2\\AI\\Project_AI\\AI_RAG_project\\myenv\\Lib\\site-packages\\httpx\\_client.py:926\u001b[0m, in \u001b[0;36msend\u001b[1;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\MediaMonster\\OneDrive\\MA2\\AI\\Project_AI\\AI_RAG_project\\myenv\\Lib\\site-packages\\httpx\\_client.py:954\u001b[0m, in \u001b[0;36m_send_handling_auth\u001b[1;34m(self, request, auth, follow_redirects, history)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\MediaMonster\\OneDrive\\MA2\\AI\\Project_AI\\AI_RAG_project\\myenv\\Lib\\site-packages\\httpx\\_client.py:991\u001b[0m, in \u001b[0;36m_send_handling_redirects\u001b[1;34m(self, request, follow_redirects, history)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\MediaMonster\\OneDrive\\MA2\\AI\\Project_AI\\AI_RAG_project\\myenv\\Lib\\site-packages\\httpx\\_client.py:1027\u001b[0m, in \u001b[0;36m_send_single_request\u001b[1;34m(self, request)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\MediaMonster\\OneDrive\\MA2\\AI\\Project_AI\\AI_RAG_project\\myenv\\Lib\\site-packages\\httpx\\_transports\\default.py:235\u001b[0m, in \u001b[0;36mhandle_request\u001b[1;34m(self, request)\u001b[0m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2032.0_x64__qbz5n2kfra8p0\\Lib\\contextlib.py:158\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[1;34m(self, typ, value, traceback)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 158\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    160\u001b[0m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\MediaMonster\\OneDrive\\MA2\\AI\\Project_AI\\AI_RAG_project\\myenv\\Lib\\site-packages\\httpx\\_transports\\default.py:89\u001b[0m, in \u001b[0;36mmap_httpcore_exceptions\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mConnectError\u001b[0m: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mAPIConnectionError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Embed the document chunks using OpenAI embeddings\u001b[39;00m\n\u001b[0;32m      2\u001b[0m embedding_model \u001b[38;5;241m=\u001b[39m OpenAIEmbeddings()\n\u001b[1;32m----> 3\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[43membedding_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpage_content\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msplits\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Initialize a FAISS index to store and search the embeddings\u001b[39;00m\n\u001b[0;32m      6\u001b[0m dimension \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(embeddings[\u001b[38;5;241m0\u001b[39m])  \u001b[38;5;66;03m# Get the dimension of the embeddings\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\MediaMonster\\OneDrive\\MA2\\AI\\Project_AI\\AI_RAG_project\\myenv\\Lib\\site-packages\\langchain_openai\\embeddings\\base.py:588\u001b[0m, in \u001b[0;36membed_documents\u001b[1;34m(self, texts, chunk_size)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\MediaMonster\\OneDrive\\MA2\\AI\\Project_AI\\AI_RAG_project\\myenv\\Lib\\site-packages\\langchain_openai\\embeddings\\base.py:483\u001b[0m, in \u001b[0;36m_get_len_safe_embeddings\u001b[1;34m(self, texts, engine, chunk_size)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\MediaMonster\\OneDrive\\MA2\\AI\\Project_AI\\AI_RAG_project\\myenv\\Lib\\site-packages\\openai\\resources\\embeddings.py:124\u001b[0m, in \u001b[0;36mcreate\u001b[1;34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\MediaMonster\\OneDrive\\MA2\\AI\\Project_AI\\AI_RAG_project\\myenv\\Lib\\site-packages\\openai\\_base_client.py:1278\u001b[0m, in \u001b[0;36mpost\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\MediaMonster\\OneDrive\\MA2\\AI\\Project_AI\\AI_RAG_project\\myenv\\Lib\\site-packages\\openai\\_base_client.py:955\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\MediaMonster\\OneDrive\\MA2\\AI\\Project_AI\\AI_RAG_project\\myenv\\Lib\\site-packages\\openai\\_base_client.py:1015\u001b[0m, in \u001b[0;36m_request\u001b[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\MediaMonster\\OneDrive\\MA2\\AI\\Project_AI\\AI_RAG_project\\myenv\\Lib\\site-packages\\openai\\_base_client.py:1093\u001b[0m, in \u001b[0;36m_retry_request\u001b[1;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\MediaMonster\\OneDrive\\MA2\\AI\\Project_AI\\AI_RAG_project\\myenv\\Lib\\site-packages\\openai\\_base_client.py:1015\u001b[0m, in \u001b[0;36m_request\u001b[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\MediaMonster\\OneDrive\\MA2\\AI\\Project_AI\\AI_RAG_project\\myenv\\Lib\\site-packages\\openai\\_base_client.py:1093\u001b[0m, in \u001b[0;36m_retry_request\u001b[1;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\MediaMonster\\OneDrive\\MA2\\AI\\Project_AI\\AI_RAG_project\\myenv\\Lib\\site-packages\\openai\\_base_client.py:1025\u001b[0m, in \u001b[0;36m_request\u001b[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n",
      "\u001b[1;31mAPIConnectionError\u001b[0m: Connection error."
     ]
    }
   ],
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> d03e42cc7351facd3f7091404dff73b6adec4af1
   "source": [
    "# Embed the document chunks using OpenAI embeddings\n",
    "embedding_model = OpenAIEmbeddings()\n",
    "embeddings = embedding_model.embed_documents([split.page_content for split in splits])\n",
    "\n",
    "# Initialize a FAISS index to store and search the embeddings\n",
    "dimension = len(embeddings[0])  # Get the dimension of the embeddings\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(np.array(embeddings, dtype='float32'))  # Add embeddings to the index\n",
    "\n",
    "print(\"Documents embedded and indexed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval function defined successfully!\n"
     ]
    }
   ],
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> d03e42cc7351facd3f7091404dff73b6adec4af1
   "source": [
    "# Function to retrieve the most similar documents for a given query\n",
    "def retrieve_similar(query, k=5):\n",
    "    query_embedding = np.array([embedding_model.embed_query(query)], dtype='float32')\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "    return [splits[i] for i in indices[0]]\n",
    "\n",
    "print(\"Retrieval function defined successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt templates defined successfully!\n"
     ]
    }
   ],
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> d03e42cc7351facd3f7091404dff73b6adec4af1
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Define different prompt templates for response generation\n",
    "general_prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"Using the information provided:\\n{context}\\nAnswer the question:\\n{question}\"\n",
    ")\n",
    "\n",
    "detailed_analysis_prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"Based on the following detailed analysis:\\n{context}\\nProvide a comprehensive answer to the question:\\n{question}\"\n",
    ")\n",
    "\n",
    "summarized_response_prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"Summarize the following content:\\n{context}\\nAnswer briefly:\\n{question}\"\n",
    ")\n",
    "\n",
    "# Dictionary to manage prompt choices\n",
    "prompt_choices = {\n",
    "    \"general\": general_prompt,\n",
    "    \"detailed\": detailed_analysis_prompt,\n",
    "    \"summary\": summarized_response_prompt\n",
    "}\n",
    "\n",
    "print(\"Prompt templates defined successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "APIConnectionError",
     "evalue": "Connection error.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConnectError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\MediaMonster\\OneDrive\\MA2\\AI\\Project_AI\\AI_RAG_project\\myenv\\Lib\\site-packages\\httpx\\_transports\\default.py:72\u001b[0m, in \u001b[0;36mmap_httpcore_exceptions\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\MediaMonster\\OneDrive\\MA2\\AI\\Project_AI\\AI_RAG_project\\myenv\\Lib\\site-packages\\httpx\\_transports\\default.py:236\u001b[0m, in \u001b[0;36mhandle_request\u001b[1;34m(self, request)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\MediaMonster\\OneDrive\\MA2\\AI\\Project_AI\\AI_RAG_project\\myenv\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:256\u001b[0m, in \u001b[0;36mhandle_request\u001b[1;34m(self, request)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\MediaMonster\\OneDrive\\MA2\\AI\\Project_AI\\AI_RAG_project\\myenv\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:236\u001b[0m, in \u001b[0;36mhandle_request\u001b[1;34m(self, request)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\MediaMonster\\OneDrive\\MA2\\AI\\Project_AI\\AI_RAG_project\\myenv\\Lib\\site-packages\\httpcore\\_sync\\connection.py:101\u001b[0m, in \u001b[0;36mhandle_request\u001b[1;34m(self, request)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\MediaMonster\\OneDrive\\MA2\\AI\\Project_AI\\AI_RAG_project\\myenv\\Lib\\site-packages\\httpcore\\_sync\\connection.py:78\u001b[0m, in \u001b[0;36mhandle_request\u001b[1;34m(self, request)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\MediaMonster\\OneDrive\\MA2\\AI\\Project_AI\\AI_RAG_project\\myenv\\Lib\\site-packages\\httpcore\\_sync\\connection.py:156\u001b[0m, in \u001b[0;36m_connect\u001b[1;34m(self, request)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\MediaMonster\\OneDrive\\MA2\\AI\\Project_AI\\AI_RAG_project\\myenv\\Lib\\site-packages\\httpcore\\_backends\\sync.py:154\u001b[0m, in \u001b[0;36mstart_tls\u001b[1;34m(self, ssl_context, server_hostname, timeout)\u001b[0m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2032.0_x64__qbz5n2kfra8p0\\Lib\\contextlib.py:158\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[1;34m(self, typ, value, traceback)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 158\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    160\u001b[0m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\MediaMonster\\OneDrive\\MA2\\AI\\Project_AI\\AI_RAG_project\\myenv\\Lib\\site-packages\\httpcore\\_exceptions.py:14\u001b[0m, in \u001b[0;36mmap_exceptions\u001b[1;34m(map)\u001b[0m\n",
      "\u001b[1;31mConnectError\u001b[0m: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mConnectError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\MediaMonster\\OneDrive\\MA2\\AI\\Project_AI\\AI_RAG_project\\myenv\\Lib\\site-packages\\openai\\_base_client.py:991\u001b[0m, in \u001b[0;36m_request\u001b[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\MediaMonster\\OneDrive\\MA2\\AI\\Project_AI\\AI_RAG_project\\myenv\\Lib\\site-packages\\httpx\\_client.py:926\u001b[0m, in \u001b[0;36msend\u001b[1;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\MediaMonster\\OneDrive\\MA2\\AI\\Project_AI\\AI_RAG_project\\myenv\\Lib\\site-packages\\httpx\\_client.py:954\u001b[0m, in \u001b[0;36m_send_handling_auth\u001b[1;34m(self, request, auth, follow_redirects, history)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\MediaMonster\\OneDrive\\MA2\\AI\\Project_AI\\AI_RAG_project\\myenv\\Lib\\site-packages\\httpx\\_client.py:991\u001b[0m, in \u001b[0;36m_send_handling_redirects\u001b[1;34m(self, request, follow_redirects, history)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\MediaMonster\\OneDrive\\MA2\\AI\\Project_AI\\AI_RAG_project\\myenv\\Lib\\site-packages\\httpx\\_client.py:1027\u001b[0m, in \u001b[0;36m_send_single_request\u001b[1;34m(self, request)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\MediaMonster\\OneDrive\\MA2\\AI\\Project_AI\\AI_RAG_project\\myenv\\Lib\\site-packages\\httpx\\_transports\\default.py:235\u001b[0m, in \u001b[0;36mhandle_request\u001b[1;34m(self, request)\u001b[0m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2032.0_x64__qbz5n2kfra8p0\\Lib\\contextlib.py:158\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[1;34m(self, typ, value, traceback)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 158\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    160\u001b[0m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\MediaMonster\\OneDrive\\MA2\\AI\\Project_AI\\AI_RAG_project\\myenv\\Lib\\site-packages\\httpx\\_transports\\default.py:89\u001b[0m, in \u001b[0;36mmap_httpcore_exceptions\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mConnectError\u001b[0m: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mAPIConnectionError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(doc\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Example: Ask a question and get a response\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrag_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat is the potability of water with these characteristics?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "File \u001b[1;32mc:\\Users\\MediaMonster\\OneDrive\\MA2\\AI\\Project_AI\\AI_RAG_project\\myenv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3022\u001b[0m, in \u001b[0;36minvoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\MediaMonster\\OneDrive\\MA2\\AI\\Project_AI\\AI_RAG_project\\myenv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3727\u001b[0m, in \u001b[0;36minvoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2032.0_x64__qbz5n2kfra8p0\\Lib\\concurrent\\futures\\_base.py:456\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    458\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2032.0_x64__qbz5n2kfra8p0\\Lib\\concurrent\\futures\\_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2032.0_x64__qbz5n2kfra8p0\\Lib\\concurrent\\futures\\thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[1;32mc:\\Users\\MediaMonster\\OneDrive\\MA2\\AI\\Project_AI\\AI_RAG_project\\myenv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3711\u001b[0m, in \u001b[0;36m_invoke_step\u001b[1;34m(step, input, config, key)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\MediaMonster\\OneDrive\\MA2\\AI\\Project_AI\\AI_RAG_project\\myenv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:4713\u001b[0m, in \u001b[0;36minvoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\MediaMonster\\OneDrive\\MA2\\AI\\Project_AI\\AI_RAG_project\\myenv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1927\u001b[0m, in \u001b[0;36m_call_with_config\u001b[1;34m(self, func, input, config, run_type, serialized, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\MediaMonster\\OneDrive\\MA2\\AI\\Project_AI\\AI_RAG_project\\myenv\\Lib\\site-packages\\langchain_core\\runnables\\config.py:396\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[1;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\MediaMonster\\OneDrive\\MA2\\AI\\Project_AI\\AI_RAG_project\\myenv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:4567\u001b[0m, in \u001b[0;36m_invoke\u001b[1;34m(self, input, run_manager, config, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\MediaMonster\\OneDrive\\MA2\\AI\\Project_AI\\AI_RAG_project\\myenv\\Lib\\site-packages\\langchain_core\\runnables\\config.py:396\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[1;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n",
      "Cell \u001b[1;32mIn[25], line 6\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(q)\u001b[0m\n\u001b[0;32m      2\u001b[0m selected_prompt \u001b[38;5;241m=\u001b[39m prompt_choices[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneral\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Define the RAG chain combining retrieval and language model generation\u001b[39;00m\n\u001b[0;32m      5\u001b[0m rag_chain \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m----> 6\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m q: format_docs(\u001b[43mretrieve_similar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m)\u001b[49m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: RunnablePassthrough()}\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;241m|\u001b[39m selected_prompt\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;241m|\u001b[39m llm\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;241m|\u001b[39m StrOutputParser()\n\u001b[0;32m     10\u001b[0m )\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Function to format the retrieved documents into a string\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mformat_docs\u001b[39m(docs):\n",
      "Cell \u001b[1;32mIn[23], line 3\u001b[0m, in \u001b[0;36mretrieve_similar\u001b[1;34m(query, k)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mretrieve_similar\u001b[39m(query, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m----> 3\u001b[0m     query_embedding \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[43membedding_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m], dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m     distances, indices \u001b[38;5;241m=\u001b[39m index\u001b[38;5;241m.\u001b[39msearch(query_embedding, k)\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [splits[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m indices[\u001b[38;5;241m0\u001b[39m]]\n",
      "File \u001b[1;32mc:\\Users\\MediaMonster\\OneDrive\\MA2\\AI\\Project_AI\\AI_RAG_project\\myenv\\Lib\\site-packages\\langchain_openai\\embeddings\\base.py:629\u001b[0m, in \u001b[0;36membed_query\u001b[1;34m(self, text)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\MediaMonster\\OneDrive\\MA2\\AI\\Project_AI\\AI_RAG_project\\myenv\\Lib\\site-packages\\langchain_openai\\embeddings\\base.py:588\u001b[0m, in \u001b[0;36membed_documents\u001b[1;34m(self, texts, chunk_size)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\MediaMonster\\OneDrive\\MA2\\AI\\Project_AI\\AI_RAG_project\\myenv\\Lib\\site-packages\\langchain_openai\\embeddings\\base.py:483\u001b[0m, in \u001b[0;36m_get_len_safe_embeddings\u001b[1;34m(self, texts, engine, chunk_size)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\MediaMonster\\OneDrive\\MA2\\AI\\Project_AI\\AI_RAG_project\\myenv\\Lib\\site-packages\\openai\\resources\\embeddings.py:124\u001b[0m, in \u001b[0;36mcreate\u001b[1;34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\MediaMonster\\OneDrive\\MA2\\AI\\Project_AI\\AI_RAG_project\\myenv\\Lib\\site-packages\\openai\\_base_client.py:1278\u001b[0m, in \u001b[0;36mpost\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\MediaMonster\\OneDrive\\MA2\\AI\\Project_AI\\AI_RAG_project\\myenv\\Lib\\site-packages\\openai\\_base_client.py:955\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\MediaMonster\\OneDrive\\MA2\\AI\\Project_AI\\AI_RAG_project\\myenv\\Lib\\site-packages\\openai\\_base_client.py:1015\u001b[0m, in \u001b[0;36m_request\u001b[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\MediaMonster\\OneDrive\\MA2\\AI\\Project_AI\\AI_RAG_project\\myenv\\Lib\\site-packages\\openai\\_base_client.py:1093\u001b[0m, in \u001b[0;36m_retry_request\u001b[1;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\MediaMonster\\OneDrive\\MA2\\AI\\Project_AI\\AI_RAG_project\\myenv\\Lib\\site-packages\\openai\\_base_client.py:1015\u001b[0m, in \u001b[0;36m_request\u001b[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\MediaMonster\\OneDrive\\MA2\\AI\\Project_AI\\AI_RAG_project\\myenv\\Lib\\site-packages\\openai\\_base_client.py:1093\u001b[0m, in \u001b[0;36m_retry_request\u001b[1;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\MediaMonster\\OneDrive\\MA2\\AI\\Project_AI\\AI_RAG_project\\myenv\\Lib\\site-packages\\openai\\_base_client.py:1025\u001b[0m, in \u001b[0;36m_request\u001b[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n",
      "\u001b[1;31mAPIConnectionError\u001b[0m: Connection error."
     ]
    }
   ],
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> d03e42cc7351facd3f7091404dff73b6adec4af1
   "source": [
    "# Select a specific prompt template for use\n",
    "selected_prompt = prompt_choices[\"general\"]\n",
    "\n",
    "# Define the RAG chain combining retrieval and language model generation\n",
    "rag_chain = (\n",
    "    {\"context\": lambda q: format_docs(retrieve_similar(q)), \"question\": RunnablePassthrough()}\n",
    "    | selected_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Function to format the retrieved documents into a string\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Example: Ask a question and get a response\n",
    "response = rag_chain.invoke(\"What is the potability of water with these characteristics?\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
<<<<<<< HEAD
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the information provided, the water in question meets the requirements of cleanliness and safety for drinking water. It is monitored for various parameters including microbiological, chemical, and indicator parameters to ensure it is safe for consumption. Therefore, the water is considered potable.\n"
     ]
    }
   ],
=======
   "outputs": [],
>>>>>>> d03e42cc7351facd3f7091404dff73b6adec4af1
   "source": [
    "# Function to format context with potability prediction\n",
    "def format_with_predictions(docs, features=None):\n",
    "    context = \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "    if features:\n",
    "        potability = predict_potability(features)\n",
    "        context += f\"\\n\\nWater Potability Prediction: {potability}\"\n",
    "    return context\n",
    "\n",
    "# Updated RAG chain that includes potability predictions\n",
    "rag_chain = (\n",
<<<<<<< HEAD
    "    #{\"context\": lambda q: format_with_predictions(retrieve_similar(q), features=[5.2, 204.5, 20791, 7.2, 333.3, 17.2, 6.2, 325.3, 0.5]), \n",
    "    {\"context\": lambda q: format_with_predictions(retrieve_similar(q), features=[5.5, 15, 0.05, 1000, 0.05, 5, 15, 5, 1000   ]),# Non-potable values\n",
=======
    "    {\"context\": lambda q: format_with_predictions(retrieve_similar(q), features=[7.2, 204.5, 20791, 7.2, 333.3, 17.2, 6.2, 325.3, 0.5]),\n",
>>>>>>> d03e42cc7351facd3f7091404dff73b6adec4af1
    "     \"question\": RunnablePassthrough()}\n",
    "    | selected_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Example: Ask a question with potability prediction\n",
    "response = rag_chain.invoke(\"What is the potability of water with these characteristics?\")\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
